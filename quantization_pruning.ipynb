{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow\n",
    "- post-training quantization\n",
    "- quantization aware training\n",
    "- weight pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String constants for model filenames\n",
    "FILE_WEIGHTS = 'baseline_weights.h5'\n",
    "FILE_NON_QUANTIZED_H5 = 'non_quantized.h5'\n",
    "FILE_NON_QUANTIZED_TFLITE = 'non_quantized.tflite'\n",
    "FILE_PT_QUANTIZED = 'post_training_quantized.tflite'\n",
    "FILE_QAT_QUANTIZED = 'quant_aware_quantized.tflite'\n",
    "FILE_PRUNED_MODEL_H5 = 'pruned_model.h5'\n",
    "FILE_PRUNED_QUANTIZED_TFLITE = 'pruned_quantized.tflite'\n",
    "FILE_PRUNED_NON_QUANTIZED_TFLITE = 'pruned_non_quantized.tflite'\n",
    "\n",
    "# Dictionaries to hold measurements\n",
    "MODEL_SIZE = {}\n",
    "ACCURACY = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metric(metric_dict, metric_name):\n",
    "  '''Prints key and values stored in a dictionary'''\n",
    "  for metric, value in metric_dict.items():\n",
    "    print(f'{metric_name} for {metric}: {value}')\n",
    "\n",
    "\n",
    "def get_gzipped_model_size(file):\n",
    "  '''Returns size of gzipped model, in bytes.'''\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder():\n",
    "  '''Returns a shallow CNN for training on the MNIST dataset'''\n",
    "\n",
    "  keras = tf.keras\n",
    "\n",
    "  # Define the model architecture.\n",
    "  model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tflite_model(filename, x_test, y_test):\n",
    "  '''\n",
    "  Measures the accuracy of a given TF Lite model and test set\n",
    "\n",
    "  Args:\n",
    "    filename (string) - filename of the model to load\n",
    "    x_test (numpy array) - test images\n",
    "    y_test (numpy array) - test labels\n",
    "\n",
    "  Returns\n",
    "    float showing the accuracy against the test set\n",
    "  '''\n",
    "\n",
    "  # Initialize the TF Lite Interpreter and allocate tensors\n",
    "  interpreter = tf.lite.Interpreter(model_path=filename)\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  # Get input and output index\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Initialize empty predictions list\n",
    "  prediction_digits = []\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  for i, test_image in enumerate(x_test):\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  accuracy = (prediction_digits == y_test).mean()\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Base Model Training\n",
    "- Output: baseline_weights.h5\n",
    "- Output: non_quantized.h5\n",
    "- Output: non_quantized.tflite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 26, 12)        120       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 12)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2028)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                20290     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,410\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the baseline model\n",
    "baseline_model = model_builder()\n",
    "\n",
    "# Save the initial weights for use later\n",
    "baseline_model.save_weights(FILE_WEIGHTS)\n",
    "\n",
    "# Print the model summary\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.2915 - accuracy: 0.9177\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.1390 - accuracy: 0.9602\n"
     ]
    }
   ],
   "source": [
    "# Setup the model for training\n",
    "baseline_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "baseline_model.fit(train_images, train_labels, epochs=1, shuffle=False)\n",
    "\n",
    "# Get the baseline accuracy\n",
    "_, ACCURACY['baseline Keras model'] = baseline_model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy for baseline Keras model: 0.9602000117301941\n",
      "model size in bytes for baseline h5: 98968\n"
     ]
    }
   ],
   "source": [
    "# Save the Keras model\n",
    "baseline_model.save(FILE_NON_QUANTIZED_H5, include_optimizer=False)\n",
    "\n",
    "# Save and get the model size\n",
    "MODEL_SIZE['baseline h5'] = os.path.getsize(FILE_NON_QUANTIZED_H5)\n",
    "\n",
    "# Print records so far\n",
    "print_metric(ACCURACY, \"test accuracy\")\n",
    "print_metric(MODEL_SIZE, \"model size in bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tflite(model, filename, quantize=False):\n",
    "    '''\n",
    "    Converts the model to TF Lite format and writes to a file\n",
    "\n",
    "    Args:\n",
    "        model (Keras model) - model to convert to TF Lite\n",
    "        filename (string) - string to use when saving the file\n",
    "        quantize (bool) - flag to indicate quantization\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    # Initialize the converter\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "    # Set for quantization if flag is set to True\n",
    "    if quantize:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    # Convert the model\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Save the model.\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\clare\\AppData\\Local\\Temp\\tmphqyiagsh\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\clare\\AppData\\Local\\Temp\\tmphqyiagsh\\assets\n"
     ]
    }
   ],
   "source": [
    "convert_tflite(baseline_model, FILE_NON_QUANTIZED_TFLITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size in bytes for baseline h5: 98968\n",
      "model size in bytes for non quantized tflite: 85012\n"
     ]
    }
   ],
   "source": [
    "MODEL_SIZE['non quantized tflite'] = os.path.getsize(FILE_NON_QUANTIZED_TFLITE)\n",
    "\n",
    "print_metric(MODEL_SIZE, 'model size in bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy for baseline Keras model: 0.9602000117301941\n",
      "test accuracy for non quantized tflite: 0.9602\n"
     ]
    }
   ],
   "source": [
    "ACCURACY['non quantized tflite'] = evaluate_tflite_model(FILE_NON_QUANTIZED_TFLITE, test_images, test_labels)\n",
    "print_metric(ACCURACY, 'test accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\clare\\AppData\\Local\\Temp\\tmp15a6bwp5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\clare\\AppData\\Local\\Temp\\tmp15a6bwp5\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size for baseline h5: 98968\n",
      "model size for non quantized tflite: 85012\n",
      "model size for post training quantized tflite: 24256\n",
      "test accuracy for baseline Keras model: 0.9602000117301941\n",
      "test accuracy for non quantized tflite: 0.9602\n",
      "test accuracy for post training quantized tflite: 0.9604\n"
     ]
    }
   ],
   "source": [
    "convert_tflite(baseline_model, FILE_PT_QUANTIZED, quantize=True)\n",
    "\n",
    "# Get the model size\n",
    "MODEL_SIZE['post training quantized tflite'] = os.path.getsize(FILE_PT_QUANTIZED)\n",
    "\n",
    "print_metric(MODEL_SIZE, 'model size')\n",
    "\n",
    "ACCURACY['post training quantized tflite'] = evaluate_tflite_model(FILE_PT_QUANTIZED, test_images, test_labels)\n",
    "\n",
    "print_metric(ACCURACY, 'test accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization Aware Training\n",
    "- Better for model accuracy\n",
    "- Lossy performance from lower precision can be solved\n",
    "- Simulates low precision behaviour in forward pass, while backward pass remains the same\n",
    "- Induces some quantization error while the optimizer tries to reduce it by adjusting params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output model is quantization model but not quantized(weights are float32 instead of int8)\n",
    "- A slight difference in the model summary compared to the baseline model summary\n",
    "- The total params count increased as expected because of the nodes added by the `quantize_model()` method\n",
    "- The method inserts fake quant nodes in the model during training and model will learn to adapt with the loss of precision to get more accurate predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLay  (None, 28, 28)           3         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " quant_reshape_1 (QuantizeWr  (None, 28, 28, 1)        1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_1 (QuantizeWra  (None, 26, 26, 12)       147       \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_max_pooling2d_1 (Quan  (None, 13, 13, 12)       1         \n",
      " tizeWrapperV2)                                                  \n",
      "                                                                 \n",
      " quant_flatten_1 (QuantizeWr  (None, 2028)             1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_1 (QuantizeWrap  (None, 10)               20295     \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,448\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 38\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# method to quantize a Keras model\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# Define the model architecture.\n",
    "model_to_quantize = model_builder()\n",
    "\n",
    "# Reinitialize weights with saved file\n",
    "model_to_quantize.load_weights(FILE_WEIGHTS)\n",
    "\n",
    "# Quantize the model\n",
    "q_aware_model = quantize_model(model_to_quantize)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 13s 6ms/step - loss: 0.2947 - accuracy: 0.9173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21dfd35d190>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_aware_model.fit(train_images, train_labels, epochs=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy for quantization aware non-quantized: 0.9591000080108643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as reshape_1_layer_call_fn, reshape_1_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op while saving (showing 5 of 9). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\clare\\AppData\\Local\\Temp\\tmp7jpwuqtu\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\clare\\AppData\\Local\\Temp\\tmp7jpwuqtu\\assets\n",
      "d:\\My Project\\mlops deeplearning-ai\\notebooks\\model_interpretability\\venv39\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy for quantization aware non-quantized: 0.9591000080108643\n",
      "test accuracy for quantization aware quantized: 0.9591\n"
     ]
    }
   ],
   "source": [
    "# Reinitialize the dictionary\n",
    "ACCURACY = {}\n",
    "\n",
    "# Get the accuracy of the quantization aware trained model (not yet quantized)\n",
    "_, ACCURACY['quantization aware non-quantized'] = q_aware_model.evaluate(test_images, test_labels, verbose=0)\n",
    "print_metric(ACCURACY, 'test accuracy')\n",
    "\n",
    "# Convert and quantize the model.\n",
    "convert_tflite(q_aware_model, FILE_QAT_QUANTIZED, quantize=True)\n",
    "\n",
    "# Get the accuracy of the quantized model\n",
    "ACCURACY['quantization aware quantized'] = evaluate_tflite_model(FILE_QAT_QUANTIZED, test_images, test_labels)\n",
    "print_metric(ACCURACY, 'test accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruning\n",
    "- This process involves zeroing out insignificant (low magnitude) weights\n",
    "- The intuition is these weights do not contribute as much to making predictions so you can remove them and get the same result\n",
    "- Making the weights sparse helps in compressing the model more efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_reshape  (None, 28, 28, 1)        1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d   (None, 26, 26, 12)       230       \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_max_poo  (None, 13, 13, 12)       1         \n",
      " ling2d (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_flatten  (None, 2028)             1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_dense (  (None, 10)               40572     \n",
      " PruneLowMagnitude)                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,805\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 20,395\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get the pruning method\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set.\n",
    "\n",
    "num_images = train_images.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define pruning schedule.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "# Pass in the trained baseline model\n",
    "model_for_pruning = prune_low_magnitude(baseline_model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 12) dtype=float32, numpy=\n",
       "array([[[[ 0.32331395,  0.37700206,  0.07158457,  0.15839554,\n",
       "          -0.5928061 ,  0.16970588,  0.27302247,  0.05167996,\n",
       "           0.1029333 , -0.29762572,  0.40509358,  0.266184  ]],\n",
       "\n",
       "        [[-0.00203653,  0.22536668,  0.16937006,  0.38190857,\n",
       "          -0.10673852,  0.1033941 ,  0.26569822,  0.21276605,\n",
       "           0.03750268, -0.6709579 ,  0.6776479 ,  0.30801836]],\n",
       "\n",
       "        [[-0.00610922,  0.02296782, -0.25250146,  0.1620334 ,\n",
       "           0.42038572,  0.2590574 ,  0.02034328, -0.15450105,\n",
       "           0.09606051, -0.4990606 ,  0.7326869 , -0.09349032]]],\n",
       "\n",
       "\n",
       "       [[[ 0.16339053,  0.44928578,  0.21899992, -0.01333159,\n",
       "          -0.6395615 ,  0.01518945,  0.3147946 ,  0.10961092,\n",
       "           0.13365772,  0.03120475, -0.32336253, -0.00616928]],\n",
       "\n",
       "        [[-0.03096809, -0.16775352,  0.24548844,  0.20115243,\n",
       "          -0.0015689 ,  0.12037516, -0.05883881,  0.2972516 ,\n",
       "           0.28075826,  0.27383828, -0.17512053, -0.01823741]],\n",
       "\n",
       "        [[ 0.21952094, -0.14761144,  0.15585393,  0.18768719,\n",
       "           0.6306595 ,  0.2547999 , -0.43827906,  0.15203363,\n",
       "           0.12652117,  0.01929338, -0.06678602,  0.01216441]]],\n",
       "\n",
       "\n",
       "       [[[-0.2292357 , -0.17402077, -0.06878596, -0.6182204 ,\n",
       "          -0.5753184 ,  0.19992226, -0.04962071,  0.11527783,\n",
       "           0.21149401,  0.45194748, -0.57181394,  0.06169656]],\n",
       "\n",
       "        [[ 0.22047226, -0.4945068 ,  0.1860784 , -0.28974733,\n",
       "           0.05471295,  0.22207604, -0.5400635 ,  0.06784433,\n",
       "          -0.02747095,  0.2993753 , -0.6191016 ,  0.2139466 ]],\n",
       "\n",
       "        [[ 0.28303817, -0.40606323, -0.09587002,  0.26370797,\n",
       "           0.52350545,  0.05399458, -0.2845225 , -0.1416291 ,\n",
       "           0.2613987 ,  0.3054392 , -0.65675986,  0.15651265]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview model weights\n",
    "model_for_pruning.weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1688/1688 [==============================] - 13s 7ms/step - loss: 0.1533 - accuracy: 0.9586 - val_loss: 0.1060 - val_accuracy: 0.9712\n",
      "Epoch 2/2\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 0.1173 - accuracy: 0.9655 - val_loss: 0.0949 - val_accuracy: 0.9757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21da62bbac0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callback to update pruning wrappers at each step\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "]\n",
    "\n",
    "# Train and prune the model\n",
    "model_for_pruning.fit(train_images, train_labels,\n",
    "                  epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 12) dtype=float32, numpy=\n",
       "array([[[[ 0.        ,  0.7375559 , -0.        ,  0.        ,\n",
       "          -0.88801235,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ,  0.        ,  0.        , -0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        , -0.        ,  0.8130417 ,\n",
       "          -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        , -1.2675618 ,  0.97723716,  0.7961744 ]],\n",
       "\n",
       "        [[ 0.        ,  0.        , -0.        ,  0.        ,\n",
       "           0.6954159 ,  0.        ,  0.        , -0.        ,\n",
       "           0.        ,  0.        ,  1.0631871 , -0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.8605922 , -0.        ,  0.        ,\n",
       "          -1.0491787 ,  0.        ,  0.        , -0.        ,\n",
       "           0.        ,  0.        ,  0.        , -0.        ]],\n",
       "\n",
       "        [[ 0.        , -0.        , -0.        ,  0.        ,\n",
       "          -0.        ,  0.        ,  0.        ,  0.8850673 ,\n",
       "           0.        ,  0.        ,  0.        , -0.        ]],\n",
       "\n",
       "        [[ 0.        , -0.        , -0.        ,  0.        ,\n",
       "           0.92360175,  0.        ,  0.        , -0.        ,\n",
       "           0.        ,  0.        ,  0.        , -0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        , -0.        , -0.75942934,\n",
       "          -0.8003828 ,  0.        ,  0.        , -0.        ,\n",
       "           0.        ,  0.8430094 , -1.1303895 , -0.        ]],\n",
       "\n",
       "        [[ 0.        , -1.0500753 , -0.        ,  0.        ,\n",
       "          -0.        ,  0.        , -0.6427481 , -0.        ,\n",
       "           0.        ,  0.7275894 , -1.095856  , -0.        ]],\n",
       "\n",
       "        [[ 0.        , -0.        , -0.        ,  0.        ,\n",
       "           0.7899386 ,  0.        ,  0.        , -0.        ,\n",
       "           0.        ,  0.        , -1.0785792 , -0.        ]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview model weights\n",
    "model_for_pruning.weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 26, 12)        120       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 12)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2028)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                20290     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,410\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Remove pruning wrappers\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "model_for_export.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 12) dtype=float32, numpy=\n",
       "array([[[[ 0.        ,  0.7375559 , -0.        ,  0.        ,\n",
       "          -0.88801235,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ,  0.        ,  0.        , -0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        , -0.        ,  0.8130417 ,\n",
       "          -0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        , -1.2675618 ,  0.97723716,  0.7961744 ]],\n",
       "\n",
       "        [[ 0.        ,  0.        , -0.        ,  0.        ,\n",
       "           0.6954159 ,  0.        ,  0.        , -0.        ,\n",
       "           0.        ,  0.        ,  1.0631871 , -0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.8605922 , -0.        ,  0.        ,\n",
       "          -1.0491787 ,  0.        ,  0.        , -0.        ,\n",
       "           0.        ,  0.        ,  0.        , -0.        ]],\n",
       "\n",
       "        [[ 0.        , -0.        , -0.        ,  0.        ,\n",
       "          -0.        ,  0.        ,  0.        ,  0.8850673 ,\n",
       "           0.        ,  0.        ,  0.        , -0.        ]],\n",
       "\n",
       "        [[ 0.        , -0.        , -0.        ,  0.        ,\n",
       "           0.92360175,  0.        ,  0.        , -0.        ,\n",
       "           0.        ,  0.        ,  0.        , -0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        , -0.        , -0.75942934,\n",
       "          -0.8003828 ,  0.        ,  0.        , -0.        ,\n",
       "           0.        ,  0.8430094 , -1.1303895 , -0.        ]],\n",
       "\n",
       "        [[ 0.        , -1.0500753 , -0.        ,  0.        ,\n",
       "          -0.        ,  0.        , -0.6427481 , -0.        ,\n",
       "           0.        ,  0.7275894 , -1.095856  , -0.        ]],\n",
       "\n",
       "        [[ 0.        , -0.        , -0.        ,  0.        ,\n",
       "           0.7899386 ,  0.        ,  0.        , -0.        ,\n",
       "           0.        ,  0.        , -1.0785792 , -0.        ]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview model weights (index 1 earlier is now 0 because pruning wrappers were removed)\n",
    "model_for_export.weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_size in bytes for baseline h5: 98968\n",
      "model_size in bytes for pruned non quantized h5: 98968\n"
     ]
    }
   ],
   "source": [
    "# Save Keras model\n",
    "model_for_export.save(FILE_PRUNED_MODEL_H5, include_optimizer=False)\n",
    "\n",
    "# Get uncompressed model size of baseline and pruned models\n",
    "MODEL_SIZE = {}\n",
    "MODEL_SIZE['baseline h5'] = os.path.getsize(FILE_NON_QUANTIZED_H5)\n",
    "MODEL_SIZE['pruned non quantized h5'] = os.path.getsize(FILE_PRUNED_MODEL_H5)\n",
    "\n",
    "print_metric(MODEL_SIZE, 'model_size in bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzipped model size in bytes for baseline h5: 77994\n",
      "gzipped model size in bytes for pruned non quantized h5: 25955\n"
     ]
    }
   ],
   "source": [
    "# Get compressed size of baseline and pruned models\n",
    "MODEL_SIZE = {}\n",
    "MODEL_SIZE['baseline h5'] = get_gzipped_model_size(FILE_NON_QUANTIZED_H5)\n",
    "MODEL_SIZE['pruned non quantized h5'] = get_gzipped_model_size(FILE_PRUNED_MODEL_H5)\n",
    "\n",
    "print_metric(MODEL_SIZE, \"gzipped model size in bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\clare\\AppData\\Local\\Temp\\tmpcj7yhq0v\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\clare\\AppData\\Local\\Temp\\tmpcj7yhq0v\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzipped model size in bytes for baseline h5: 77994\n",
      "gzipped model size in bytes for pruned non quantized h5: 25955\n",
      "gzipped model size in bytes for pruned quantized tflite: 8235\n"
     ]
    }
   ],
   "source": [
    "# Convert and quantize the pruned model.\n",
    "pruned_quantized_tflite = convert_tflite(model_for_export, FILE_PRUNED_QUANTIZED_TFLITE, quantize=True)\n",
    "\n",
    "# Compress and get the model size\n",
    "MODEL_SIZE['pruned quantized tflite'] = get_gzipped_model_size(FILE_PRUNED_QUANTIZED_TFLITE)\n",
    "print_metric(MODEL_SIZE, \"gzipped model size in bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1055 - accuracy: 0.9676\n",
      "accuracy for pruned model h5: 0.9675999879837036\n",
      "accuracy for pruned and quantized tflite: 0.9683\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy of pruned Keras and TF Lite models\n",
    "ACCURACY = {}\n",
    "\n",
    "_, ACCURACY['pruned model h5'] = model_for_pruning.evaluate(test_images, test_labels)\n",
    "ACCURACY['pruned and quantized tflite'] = evaluate_tflite_model(FILE_PRUNED_QUANTIZED_TFLITE, test_images, test_labels)\n",
    "\n",
    "print_metric(ACCURACY, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
