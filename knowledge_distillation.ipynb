{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knowledge Distillation (Teacher & Student Network)\n",
    "\n",
    "<!-- ![Image Title](image/overview.webp)\n",
    " -->\n",
    "\n",
    " <img src=\"image/overview.webp\" alt=\"OpenAI Logo\" style=\"width: 1000px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Components:\n",
    " <img src=\"image/knowledge.webp\" alt=\"OpenAI Logo\" style=\"width: 500px;\"/>\n",
    "\n",
    "- Knowledge (learned weights and biases): Normally the logits (raw score generated by last linear layer) as the source / weights or activations\n",
    "\n",
    "- <img src=\"image/response based knowledge.webp\" alt=\"OpenAI Logo\" style=\"width: 300px;\"/>\n",
    "- Respones based : Student model will learn to mimic the predictions of the teacher model using loss function (distillation loss)\n",
    " \n",
    "- <img src=\"image/feature based knowledge.webp\" alt=\"OpenAI Logo\" style=\"width: 300px;\"/>\n",
    "- Feature based : Intermesdiate layers laern to discrimiate specific features and this knowledge can be used to train a student model (learn some features activations)\n",
    "\n",
    "- <img src=\"image/relation based knowledge.webp\" alt=\"OpenAI Logo\" style=\"width: 300px;\"/>\n",
    "- Relation based : Relationship between feature maps is used to train student model, modelled as correlation between feature maps, graphs, similariy matrix, feature embeddings or probabilistic distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "- <img src=\"image/training.webp\" alt=\"OpenAI Logo\" style=\"width: 300px;\"/>\n",
    "- Offline Distillation : pre-trained teacher model is used to guide the student model\n",
    "- Online Distillation : both the teacher and student models are updated simultaneously in a single end-to-end training process\n",
    "- Self-Distillation : same model is used for the teacher and the student models. For instance, knowledge from deeper layers of a deep neural network can be used to train the shallow layers. Knowledge from earlier epochs of the teacher model can be transferred to its later epochs to train the student model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "\n",
    "The design of the student-teacher network architecture is critical for efficient knowledge acquisition and distillation. Typically, there is a model capacity gap between the more complex teacher model and the simpler student model. This structural gap can be reduced through optimizing knowledge transfer via efficient student-teacher architectures.\n",
    "\n",
    "Transferring knowledge from deep neural networks is not straightforward due to their depth as well as breadth. The most common architectures for knowledge transfer include a student model that is: \n",
    "\n",
    "- a shallower version of the teacher model with fewer layers and fewer neurons per layer,\n",
    "- a quantized version of the teacher model, \n",
    "- a smaller network with efficient basic operations,\n",
    "- a smaller networks with optimized global network architecture, the same model as the teacher. \n",
    "\n",
    "In addition to the above methods, recent advances like neural architecture search can also be employed for designing an optimal student model architecture given a particular teacher model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithms \n",
    "\n",
    "1. Adversarial Distillation: Generator model that learns to genrate synthetic data samples and also a discriminator to differentite the samples from the student and the teach models based on either logits or feature maps.\n",
    "\n",
    "2. Multi-Teacher Distillation: Using an ensemble of teacher models can provide the student model with distinct kinds of knowledge that can be more beneficial than knowledge acquired from a single teacher model.\n",
    "\n",
    "3. Cross-modal Distillation: The teacher is trained in one modality and its knowledge is distilled into the student that requires knowledge from a different modality. This situation arises when data or labels are not available for specific modalities either during training or testing thus necessitating the need to transfer knowledge across modalities. Cross-modal distillation is used most commonly in the visual domain. For example, the knowledge from a teacher trained on labeled image data can be used for distillation for a student model with an unlabeled input domain like optical flow or text or audio. In this case, features learned from the images from the teacher model are used for supervised training of the student model. Cross-modal distillation is useful for applications like visual question answering, image captioning amongst others.\n",
    "\n",
    "4. Others: Graph-based distillation / Attention-based distillation / Data-free distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, MaxPool2D, BatchNormalization, Input, Conv2DTranspose, Concatenate\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import h5py\n",
    "from IPython.display import display\n",
    "from PIL import Image as im\n",
    "import datetime\n",
    "import random\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
